##Pre-Processing

import os

import pandas as pd
import numpy as np
import h5py

from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.utils import shuffle


def get_service_list(dirname='list', filename='service.txt'):
    print('Getting service list...')
    with open(os.path.join(dirname, filename), 'r') as service:
        service_list = service.read().split('\n')
    return service_list


def get_flag_list(dirname='list', filename='flag.txt'):
    print('Getting flag list...')
    with open(os.path.join(dirname, filename), 'r') as flag:
        flag_list = flag.read().split('\n')
    return flag_list


def get_data_frame(dirname='dataset', filename=None):
    if filename is None:
        raise ValueError('File name should be set.')
    print('Getting data frame from a file...')
    print('Reading file:', os.path.join(dirname, filename))
    df = pd.read_csv(os.path.join(dirname, filename), header=None)
    return df


def to_numeric(data_frame, service_list, flag_list, test=False, attack=False, save=True):
    if test and attack:
        raise ValueError('Test data cannot have attack logs.')
    df = data_frame

    if not test:
        if not attack:
            # extract only normal data
            print('Data size before normal data extraction:', df.shape)
            # index 41: label (index number starts from 0)
            df = df[df[41] == 'normal'].copy()
            print('Data size after normal data extraction:', df.shape)
        else:
            # extract 99% normal data and 1% attack data
            print('Data size before normal data extraction:', df.shape)
            # index 41: label (index number starts from 0)
            df_normal_copy = df[df[41] == 'normal'].copy()
            df_normal = df_normal_copy.sample(n=int(df_normal_copy.shape[0] * 0.99), random_state=1398)
            df_attack = df[df[41] != 'normal'].sample(n=int(df_normal_copy.shape[0] * 0.01) + 1, random_state=1398)
            print('Number of normal records:', len(df_normal))
            print('Number of attack records:', len(df_attack))
            df = pd.concat((df_normal, df_attack))
            print('Data size of concatenated data:', df.shape)
        df = shuffle(df, random_state=1398)

    # index 1: protocol_type
    print('Replacing protocol_type values to numeric...')
    df[1].replace(['tcp', 'udp', 'icmp'], range(3), inplace=True)

    # index 2: service
    print('Replacing service values to numeric...')
    df[2].replace(service_list, range(len(service_list)), inplace=True)

    # index 3: flag
    print('Replacing flag values to numeric...')
    df[3].replace(flag_list, range(len(flag_list)), inplace=True)

    if not test:
        # extract only the same features from Kyoto 2006+ dataset
        df = df.loc[:, [0, 1, 2, 3, 4, 5, 22, 24, 25, 28, 31, 32, 35, 37, 38]]
    else:
        # include label
        df = df.loc[:, [0, 1, 2, 3, 4, 5, 22, 24, 25, 28, 31, 32, 35, 37, 38, 41]]
        df[41] = df[41].map(lambda x: 0 if x == 'normal' else 1)  # normal 0, attack 1

    # save as csv file
    if save:
        if not os.path.exists('csv'):
            os.makedirs('csv')
        if not test:
            if not attack:
                print('Saving file:', os.path.join('csv', 'train_normal_numeric.csv'))
                df.to_csv(os.path.join('csv', 'train_normal_numeric.csv'))
            else:
                print('Saving file:', os.path.join('csv', 'train_mixed_numeric.csv'))
                df.to_csv(os.path.join('csv', 'train_mixed_numeric.csv'))
        else:
            print('Saving file:', os.path.join('csv', 'test_numeric.csv'))
            df.to_csv(os.path.join('csv', 'test_numeric.csv'))

    return df


def to_machine_readable(data_frame, service_list, flag_list, test=False, attack=False, save_csv=True):
    if test and attack:
        raise ValueError('Test data cannot have attack logs.')
    df = data_frame
    sc = MinMaxScaler()
    enc = OneHotEncoder(categories=[range(3), range(len(service_list)), range(len(flag_list))])
    num_desc = df.loc[:, [0, 4, 5]].describe()

    # extract and drop label
    label, df_label = [], []
    if test:
        label = df[41].copy().values.reshape((df.shape[0], 1))
        df_label = pd.DataFrame(label)
        df.drop([41], axis=1, inplace=True)

    # index 0, 4, 5: duration, src_bytes, dst_bytes (in kyoto: index 0, 2, 3)
    attr_name = ['duration', '', '', '', 'src_bytes', 'dst_bytes']
    for i in [0, 4, 5]:
        print('Converting {0} data (index {1}) to machine readable...'.format(attr_name[i], i))
        iqr = (num_desc[i].values[6] - num_desc[i].values[4])
        std = num_desc[i].values[6] + iqr * 1.5  # IQR upper fence = Q3 + 1.5 * IQR
        if std == 0:
            df[i] = df[i].map(lambda x: 1 if x > 0 else 0)
        else:
            df[i] = df[i].map(lambda x: std if x > std else x)
    sc.fit(df[[0, 4, 5]].values)
    df[[0, 4, 5]] = sc.transform(df[[0, 4, 5]].values)

    # index 22, 31, 32: count, dst_host_count, dst_host_srv_count (in kyoto: index 4, 8, 9)
    print('Converting count data (index 22, 31, 32) to machine readable...')
    sc.fit(df[[22, 31, 32]].values.astype(np.float32))
    df[[22, 31, 32]] = sc.transform(df[[22, 31, 32]].values.astype(np.float32))

    # index 1, 2, 3: protocol_type, service, flag (in kyoto: index 23, 1, 13)
    print('Converting type data (index 1, 2, 3) to machine readable...')
    enc.fit(df[[1, 2, 3]].values)
    one_hot_arr = enc.transform(df[[1, 2, 3]].values).toarray()

    # drop one-hot data and attach it again
    print('Dropping and attaching one-hot encoding data...')
    df.drop([1, 2, 3], axis=1, inplace=True)
    df_final = np.concatenate((df.values, one_hot_arr), axis=1)
    df_final = pd.DataFrame(df_final)

    # print shape of data frame
    print('Final shape of data:', df_final.shape)
    if test:
        print('Final shape of label', df_label.shape)

    # save data frame into csv format
    if save_csv:
        if not os.path.exists('csv'):
            os.makedirs('csv')
        if not test:
            if not attack:
                print('Saving file:', os.path.join('csv', 'train_normal_final.csv'))
                df_final.to_csv(os.path.join('csv', 'train_normal_final.csv'), index=False)
            else:
                print('Saving file:', os.path.join('csv', 'train_mix_final.csv'))
                df_final.to_csv(os.path.join('csv', 'train_mix_final.csv'), index=False)
        else:
            print('Saving file:', os.path.join('csv', 'test_feature_final.csv'))
            df_final.to_csv(os.path.join('csv', 'test_feature_final.csv'), index=False)
            print('Saving file:', os.path.join('csv', 'test_label_final.csv'))
            df_label.to_csv(os.path.join('csv', 'test_label_final.csv'), index=False)

    # save into hdf5 format
    if not os.path.exists(os.path.join('..', 'hdf5')):
        os.makedirs(os.path.join('..', 'hdf5'))
    if not test:
        if not attack:
            with h5py.File(os.path.join('..', 'hdf5', 'train_normal.hdf5'), 'w') as hdf:
                print('Saving file:', os.path.join('..', 'hdf5', 'train_normal.hdf5'))
                hdf['x'] = df_final.values[:]
        else:
            with h5py.File(os.path.join('..', 'hdf5', 'train_mix.hdf5'), 'w') as hdf:
                print('Saving file:', os.path.join('..', 'hdf5', 'train_mix.hdf5'))
                hdf['x'] = df_final.values[:]
    else:
        with h5py.File(os.path.join('..', 'hdf5', 'test.hdf5'), 'w') as hdf:
            print('Saving file:', os.path.join('..', 'hdf5', 'test.hdf5'))
            hdf['x'] = df_final.values[:]
            hdf['y'] = df_label[:]

## Main file
from preprocess import *

print('NSL-KDD dataset preprocessor', end='\n\n')
service_list = get_service_list(dirname='list', filename='service.txt')
flag_list = get_flag_list(dirname='list', filename='flag.txt')

# 100% normal training data
df_train = get_data_frame(dirname='dataset', filename='KDDTrain+.txt')
df_train = to_numeric(df_train, service_list, flag_list)
to_machine_readable(df_train, service_list, flag_list)
print('Train data preprocess finished!', end='\n\n')

# 1% attack mixed training data
df_train_atk = get_data_frame(dirname='dataset', filename='KDDTrain+.txt')
df_train_atk = to_numeric(df_train_atk, service_list, flag_list, attack=True)
to_machine_readable(df_train_atk, service_list, flag_list, attack=True)
print('Train data with attack preprocess finished!', end='\n\n')

# test data
df_test = get_data_frame(dirname='dataset', filename='KDDTest+.txt')
df_test = to_numeric(df_test, service_list, flag_list, test=True)
to_machine_readable(df_test, service_list, flag_list, test=True)
print('Test data preprocess finished!', end='\n\n')

##Model
Import required packages
import os
import itertools

import numpy as np
import pandas as pd
import h5py

from keras.layers import Input, Dense, LeakyReLU, Dropout
from keras.models import Model, load_model
from keras.callbacks.callbacks import EarlyStopping, ModelCheckpoint

import matplotlib.pyplot as plt

Dataset import function

def get_dataset(dirname=os.path.join('..', 'hdf5'), filename=None, tagname=None):
    if filename is None or tagname is None:
        raise ValueError('File name and tag name should be set.')
    with h5py.File(os.path.join(dirname, filename), 'r') as hdf:
        return hdf[tagname][:]

Autoencoder intialization function

def get_autoencoder(loss_func):    
    # input layer
    inp = Input(shape=(114,)) # layer 1
    
    # encoding layers
    enc = Dense(512)(inp) # layer 2
    enc = Dropout(0.5)(enc)
    enc = LeakyReLU(0.01)(enc)
    enc = Dense(256)(enc) # layer 3
    enc = Dropout(0.5)(enc)
    enc = LeakyReLU(0.01)(enc)
    enc = Dense(128)(enc) # layer 4
    enc = Dropout(0.5)(enc)
    enc = LeakyReLU(0.01)(enc)
    
    # bottleneck layer
    mid = Dense(64)(enc) # layer 5
    mid = Dropout(0.5)(mid)
     mid = LeakyReLU(0.01)(mid)
    
    # decoding layers
    dec = Dense(128)(mid) # layer 6
    dec = Dropout(0.5)(dec)
    dec = LeakyReLU(0.01)(dec)
    dec = Dense(256)(dec) # layer 7
    dec = Dropout(0.5)(dec)
    dec = LeakyReLU(0.01)(dec)
    dec = Dense(512)(dec) # layer 8
    dec = Dropout(0.5)(dec)
    dec = LeakyReLU(0.01)(dec)
    
    # output layer
    out = Dense(114, activation='sigmoid')(dec)
    
    # create new model
    autoencoder = Model(inp, out)
    autoencoder.compile(loss=loss_func, optimizer='rmsprop')
    return autoencoder
    
   # Model summary
get_autoencoder('categorical_crossentropy').summary()

##Train function
def train_and_save(loss_func, train_type, filename):
    # get dataset
    if train_type == 'normal':
        x_train = get_dataset(filename='train_normal.hdf5', tagname='x')
    elif train_type == 'mix':
        x_train = get_dataset(filename='train_mix.hdf5', tagname='x')
    else:
        raise ValueError('Train type should be either normal or mix.')
    x_val = get_dataset(filename='test.hdf5', tagname='x')
    
    # autoencoder initialization
    autoencoder = get_autoencoder(loss_func)
    
    # set callbacks
    es = EarlyStopping(patience=3, monitor='val_loss')
    mc = ModelCheckpoint(os.path.join('save', filename), save_best_only=True)
    
    # train
    history = autoencoder.fit(x_train, x_train, validation_data=(x_val, x_val),
                    epochs=30, batch_size=64, callbacks=[es, mc])
    print('Best model saved to', os.path.join('save', filename), end='\n\n')
    
    return history
    
    ##Start training
    model_list = list()
history_list = list()
for loss_func, train_type in itertools.product(loss_func_list, train_type_list):
    model_list.append('_'.join([loss_func, train_type]) + '.h5')
    print('Training', model_list[-1], '...')
    history = train_and_save(loss_func, train_type, model_list[-1])
    history_list.append(history)
    
    fig, ax = plt.subplots(2, 2, figsize=(16, 10))
for i, h in enumerate(history_list):
    axx = ax[i%2][i//2]
    axx.grid(linestyle='--')
    axx.plot(h.history['loss'])
    axx.plot(h.history['val_loss'])
    axx.set_title(model_list[i][:-3])
    axx.legend(['loss', 'val_loss'], loc='upper left')
    axx.set_xlabel('Epoch')
    axx.set_ylabel('Loss')
